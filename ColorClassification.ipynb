{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColorClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cuillerj/ApeRobot/blob/master/ColorClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f92-4Hjy7kA8"
      },
      "source": [
        "<a href=\"https://www.arduino.cc/\"><img src=\"https://raw.githubusercontent.com/sandeepmistry/aimldevfest-workshop-2019/master/images/Arduino_logo_R_highquality.png\" width=200/></a>\n",
        "# Tiny ML on Arduino\n",
        "## Classify objects by color tutorial\n",
        "\n",
        " \n",
        "https://github.com/arduino/ArduinoTensorFlowLiteTutorials/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn-m5Roxnq0e"
      },
      "source": [
        "This is based on FruitToEmoji-GIT.ipynb\n",
        "The aim is to classify colors with deep learning with an NANO 33 BLE sense\n",
        "I added the clear data to the 3 red blue green in the capture file\n",
        "I added one layer to the learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment \n",
        "\n",
        "The next cell sets up the dependencies in required for the notebook, run it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNtUZdh5ncrR"
      },
      "source": [
        "requires tensorflow version 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2gs-PL4xDkZ",
        "outputId": "e07af835-ec98-4443-da61-8b1ec0dc006b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/v/vim/xxd_8.0.1453-1ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (46.0.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0rc1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0rc0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "# Upload Data\n",
        "\n",
        "1. Open the panel on the left side of Colab by clicking on the __>__\n",
        "1. Select the Files tab\n",
        "1. Drag `csv` files from your computer to the tab to upload them into colab /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qla2BR_OvHDD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxUeYPNQbOg"
      },
      "source": [
        "# Train Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the full connected neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGChd1FAk5_j",
        "outputId": "77362955-af32-4c81-b6b8-32afb92dd3e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import fileinput\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "CLASSES = [];\n",
        "\n",
        "for file in os.listdir(\"/content/\"):\n",
        "    if file.endswith(\".csv\"):\n",
        "        CLASSES.append(os.path.splitext(file)[0])\n",
        "\n",
        "CLASSES.sort()\n",
        "\n",
        "SAMPLES_WINDOW_LEN = 1\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "print(f\"Nb classes= {NUM_CLASSES}\\n\")\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for class_index in range(NUM_CLASSES):\n",
        "  objectClass = CLASSES[class_index]\n",
        "  df = pd.read_csv(\"/content/\" + objectClass + \".csv\")\n",
        "  columns = list(df)\n",
        "  # get rid of pesky empty value lines of csv which cause NaN inputs to TensorFlow\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "   \n",
        "  # calculate the number of objectClass recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_WINDOW_LEN)\n",
        "  print(f\"\\u001b[32;4m{objectClass}\\u001b[0m class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier\")\n",
        "  print(f\"{num_recordings} samples captured for training with inputs {list(df)} \\n\")\n",
        "\n",
        "  # graphing\n",
        "  plt.rcParams[\"figure.figsize\"] = (10,1)\n",
        "  pixels = np.array([df['Red'],df['Green'],df['Blue']],float)\n",
        "  pixels = np.transpose(pixels)\n",
        "  for i in range(num_recordings):\n",
        "    plt.axvline(x=i, linewidth=8, color=tuple(pixels[i]/np.max(pixels[i], axis=0)))\n",
        "  plt.show()\n",
        "  \n",
        "  #tensors\n",
        "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    row = []\n",
        "    for c in columns:\n",
        "      row.append(df[c][i])\n",
        "    tensor += row\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")\n",
        "\n",
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version = 2.2.0-rc1\n",
            "\n",
            "Nb classes= 4\n",
            "\n",
            "\u001b[32;4mblue\u001b[0m class will be output \u001b[32m0\u001b[0m of the classifier\n",
            "241 samples captured for training with inputs ['Red', 'Green', 'Blue', 'Lux'] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABZCAYAAAAaRaGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n22shAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32;4mgreen\u001b[0m class will be output \u001b[32m1\u001b[0m of the classifier\n",
            "228 samples captured for training with inputs ['Red', 'Green', 'Blue', 'Lux'] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABZCAYAAAAaRaGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNfwfx6ZjsesV+uAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32;4mred\u001b[0m class will be output \u001b[32m2\u001b[0m of the classifier\n",
            "286 samples captured for training with inputs ['Red', 'Green', 'Blue', 'Lux'] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABZCAYAAAAaRaGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32;4myellow\u001b[0m class will be output \u001b[32m3\u001b[0m of the classifier\n",
            "286 samples captured for training with inputs ['Red', 'Green', 'Blue', 'Lux'] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABZCAYAAAAaRaGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgAAaOD/AYtgYFRdsf2nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Data set parsing and preparation complete.\n",
            "Data set randomization and splitting complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8qlSAX1b6Yv"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGNFa-lX24Qo",
        "outputId": "bf195e65-da72-4fe1-bf7f-576f09122fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one class to occur per input\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit(inputs_train, outputs_train, epochs=400, batch_size=4, validation_data=(inputs_validate, outputs_validate))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1862 - mae: 0.3736 - val_loss: 0.1835 - val_mae: 0.3708\n",
            "Epoch 2/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1785 - mae: 0.3651 - val_loss: 0.1709 - val_mae: 0.3563\n",
            "Epoch 3/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1627 - mae: 0.3450 - val_loss: 0.1539 - val_mae: 0.3336\n",
            "Epoch 4/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1457 - mae: 0.3198 - val_loss: 0.1384 - val_mae: 0.3087\n",
            "Epoch 5/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1308 - mae: 0.2931 - val_loss: 0.1308 - val_mae: 0.2916\n",
            "Epoch 6/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1205 - mae: 0.2725 - val_loss: 0.1199 - val_mae: 0.2714\n",
            "Epoch 7/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.2591 - val_loss: 0.1131 - val_mae: 0.2583\n",
            "Epoch 8/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1067 - mae: 0.2466 - val_loss: 0.1110 - val_mae: 0.2543\n",
            "Epoch 9/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.1023 - mae: 0.2397 - val_loss: 0.1024 - val_mae: 0.2401\n",
            "Epoch 10/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0969 - mae: 0.2297 - val_loss: 0.1014 - val_mae: 0.2369\n",
            "Epoch 11/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0928 - mae: 0.2225 - val_loss: 0.0968 - val_mae: 0.2285\n",
            "Epoch 12/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0890 - mae: 0.2146 - val_loss: 0.0905 - val_mae: 0.2173\n",
            "Epoch 13/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0853 - mae: 0.2072 - val_loss: 0.0889 - val_mae: 0.2118\n",
            "Epoch 14/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0819 - mae: 0.1992 - val_loss: 0.0929 - val_mae: 0.2134\n",
            "Epoch 15/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0796 - mae: 0.1936 - val_loss: 0.0814 - val_mae: 0.1974\n",
            "Epoch 16/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0776 - mae: 0.1886 - val_loss: 0.0811 - val_mae: 0.1939\n",
            "Epoch 17/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0748 - mae: 0.1822 - val_loss: 0.0791 - val_mae: 0.1872\n",
            "Epoch 18/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0733 - mae: 0.1771 - val_loss: 0.0764 - val_mae: 0.1824\n",
            "Epoch 19/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1734 - val_loss: 0.0746 - val_mae: 0.1771\n",
            "Epoch 20/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0703 - mae: 0.1696 - val_loss: 0.0745 - val_mae: 0.1759\n",
            "Epoch 21/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0695 - mae: 0.1663 - val_loss: 0.0727 - val_mae: 0.1718\n",
            "Epoch 22/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0678 - mae: 0.1618 - val_loss: 0.0718 - val_mae: 0.1678\n",
            "Epoch 23/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0681 - mae: 0.1602 - val_loss: 0.0708 - val_mae: 0.1635\n",
            "Epoch 24/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0666 - mae: 0.1566 - val_loss: 0.0742 - val_mae: 0.1686\n",
            "Epoch 25/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0661 - mae: 0.1550 - val_loss: 0.0702 - val_mae: 0.1616\n",
            "Epoch 26/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0658 - mae: 0.1531 - val_loss: 0.0719 - val_mae: 0.1626\n",
            "Epoch 27/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0650 - mae: 0.1504 - val_loss: 0.0696 - val_mae: 0.1585\n",
            "Epoch 28/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0650 - mae: 0.1489 - val_loss: 0.0698 - val_mae: 0.1578\n",
            "Epoch 29/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0637 - mae: 0.1454 - val_loss: 0.0681 - val_mae: 0.1542\n",
            "Epoch 30/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0637 - mae: 0.1449 - val_loss: 0.0687 - val_mae: 0.1540\n",
            "Epoch 31/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0614 - mae: 0.1408 - val_loss: 0.0654 - val_mae: 0.1480\n",
            "Epoch 32/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0619 - mae: 0.1411 - val_loss: 0.0675 - val_mae: 0.1483\n",
            "Epoch 33/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0635 - mae: 0.1416 - val_loss: 0.0681 - val_mae: 0.1514\n",
            "Epoch 34/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0617 - mae: 0.1388 - val_loss: 0.0729 - val_mae: 0.1565\n",
            "Epoch 35/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0611 - mae: 0.1375 - val_loss: 0.0640 - val_mae: 0.1441\n",
            "Epoch 36/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0605 - mae: 0.1358 - val_loss: 0.0653 - val_mae: 0.1454\n",
            "Epoch 37/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0606 - mae: 0.1344 - val_loss: 0.0639 - val_mae: 0.1432\n",
            "Epoch 38/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0604 - mae: 0.1340 - val_loss: 0.0622 - val_mae: 0.1401\n",
            "Epoch 39/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0600 - mae: 0.1334 - val_loss: 0.0626 - val_mae: 0.1398\n",
            "Epoch 40/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0593 - mae: 0.1314 - val_loss: 0.0626 - val_mae: 0.1394\n",
            "Epoch 41/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0602 - mae: 0.1314 - val_loss: 0.0679 - val_mae: 0.1458\n",
            "Epoch 42/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0589 - mae: 0.1289 - val_loss: 0.0621 - val_mae: 0.1378\n",
            "Epoch 43/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0594 - mae: 0.1297 - val_loss: 0.0604 - val_mae: 0.1343\n",
            "Epoch 44/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0589 - mae: 0.1283 - val_loss: 0.0609 - val_mae: 0.1353\n",
            "Epoch 45/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0587 - mae: 0.1278 - val_loss: 0.0606 - val_mae: 0.1331\n",
            "Epoch 46/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0582 - mae: 0.1262 - val_loss: 0.0652 - val_mae: 0.1403\n",
            "Epoch 47/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0577 - mae: 0.1255 - val_loss: 0.0600 - val_mae: 0.1325\n",
            "Epoch 48/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0576 - mae: 0.1243 - val_loss: 0.0607 - val_mae: 0.1282\n",
            "Epoch 49/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0575 - mae: 0.1241 - val_loss: 0.0605 - val_mae: 0.1329\n",
            "Epoch 50/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0580 - mae: 0.1239 - val_loss: 0.0586 - val_mae: 0.1297\n",
            "Epoch 51/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0566 - mae: 0.1229 - val_loss: 0.0582 - val_mae: 0.1280\n",
            "Epoch 52/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0574 - mae: 0.1224 - val_loss: 0.0575 - val_mae: 0.1274\n",
            "Epoch 53/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0567 - mae: 0.1216 - val_loss: 0.0621 - val_mae: 0.1337\n",
            "Epoch 54/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0564 - mae: 0.1210 - val_loss: 0.0595 - val_mae: 0.1256\n",
            "Epoch 55/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0565 - mae: 0.1201 - val_loss: 0.0585 - val_mae: 0.1287\n",
            "Epoch 56/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0559 - mae: 0.1199 - val_loss: 0.0564 - val_mae: 0.1245\n",
            "Epoch 57/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0556 - mae: 0.1195 - val_loss: 0.0584 - val_mae: 0.1281\n",
            "Epoch 58/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0559 - mae: 0.1193 - val_loss: 0.0559 - val_mae: 0.1232\n",
            "Epoch 59/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0554 - mae: 0.1175 - val_loss: 0.0569 - val_mae: 0.1219\n",
            "Epoch 60/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0551 - mae: 0.1174 - val_loss: 0.0552 - val_mae: 0.1216\n",
            "Epoch 61/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0550 - mae: 0.1163 - val_loss: 0.0558 - val_mae: 0.1240\n",
            "Epoch 62/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0547 - mae: 0.1165 - val_loss: 0.0561 - val_mae: 0.1240\n",
            "Epoch 63/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0534 - mae: 0.1138 - val_loss: 0.0582 - val_mae: 0.1267\n",
            "Epoch 64/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0545 - mae: 0.1158 - val_loss: 0.0621 - val_mae: 0.1326\n",
            "Epoch 65/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0543 - mae: 0.1155 - val_loss: 0.0539 - val_mae: 0.1188\n",
            "Epoch 66/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0547 - mae: 0.1153 - val_loss: 0.0532 - val_mae: 0.1179\n",
            "Epoch 67/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0532 - mae: 0.1121 - val_loss: 0.0560 - val_mae: 0.1229\n",
            "Epoch 68/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0538 - mae: 0.1133 - val_loss: 0.0536 - val_mae: 0.1188\n",
            "Epoch 69/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0537 - mae: 0.1123 - val_loss: 0.0526 - val_mae: 0.1158\n",
            "Epoch 70/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0535 - mae: 0.1125 - val_loss: 0.0528 - val_mae: 0.1179\n",
            "Epoch 71/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0531 - mae: 0.1127 - val_loss: 0.0531 - val_mae: 0.1147\n",
            "Epoch 72/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0530 - mae: 0.1116 - val_loss: 0.0586 - val_mae: 0.1260\n",
            "Epoch 73/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0528 - mae: 0.1111 - val_loss: 0.0512 - val_mae: 0.1148\n",
            "Epoch 74/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0525 - mae: 0.1116 - val_loss: 0.0521 - val_mae: 0.1168\n",
            "Epoch 75/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0510 - mae: 0.1090 - val_loss: 0.0517 - val_mae: 0.1140\n",
            "Epoch 76/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0517 - mae: 0.1096 - val_loss: 0.0521 - val_mae: 0.1178\n",
            "Epoch 77/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0509 - mae: 0.1095 - val_loss: 0.0503 - val_mae: 0.1135\n",
            "Epoch 78/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0505 - mae: 0.1092 - val_loss: 0.0527 - val_mae: 0.1124\n",
            "Epoch 79/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0495 - mae: 0.1077 - val_loss: 0.0505 - val_mae: 0.1150\n",
            "Epoch 80/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0494 - mae: 0.1072 - val_loss: 0.0510 - val_mae: 0.1153\n",
            "Epoch 81/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0502 - mae: 0.1080 - val_loss: 0.0485 - val_mae: 0.1098\n",
            "Epoch 82/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0496 - mae: 0.1071 - val_loss: 0.0545 - val_mae: 0.1193\n",
            "Epoch 83/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0477 - mae: 0.1045 - val_loss: 0.0490 - val_mae: 0.1102\n",
            "Epoch 84/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0491 - mae: 0.1046 - val_loss: 0.0480 - val_mae: 0.1106\n",
            "Epoch 85/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0494 - mae: 0.1059 - val_loss: 0.0493 - val_mae: 0.1126\n",
            "Epoch 86/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0484 - mae: 0.1056 - val_loss: 0.0537 - val_mae: 0.1182\n",
            "Epoch 87/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0482 - mae: 0.1044 - val_loss: 0.0476 - val_mae: 0.1090\n",
            "Epoch 88/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0485 - mae: 0.1046 - val_loss: 0.0466 - val_mae: 0.1071\n",
            "Epoch 89/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0480 - mae: 0.1042 - val_loss: 0.0462 - val_mae: 0.1059\n",
            "Epoch 90/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0471 - mae: 0.1034 - val_loss: 0.0495 - val_mae: 0.1125\n",
            "Epoch 91/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0469 - mae: 0.1034 - val_loss: 0.0447 - val_mae: 0.1054\n",
            "Epoch 92/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0469 - mae: 0.1024 - val_loss: 0.0449 - val_mae: 0.1068\n",
            "Epoch 93/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0463 - mae: 0.1026 - val_loss: 0.0442 - val_mae: 0.1051\n",
            "Epoch 94/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0452 - mae: 0.1013 - val_loss: 0.0437 - val_mae: 0.1052\n",
            "Epoch 95/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0442 - mae: 0.0998 - val_loss: 0.0430 - val_mae: 0.1027\n",
            "Epoch 96/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0443 - mae: 0.0987 - val_loss: 0.0450 - val_mae: 0.1074\n",
            "Epoch 97/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0437 - mae: 0.0978 - val_loss: 0.0424 - val_mae: 0.1016\n",
            "Epoch 98/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0434 - mae: 0.0983 - val_loss: 0.0419 - val_mae: 0.1001\n",
            "Epoch 99/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0435 - mae: 0.0983 - val_loss: 0.0414 - val_mae: 0.1004\n",
            "Epoch 100/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0425 - mae: 0.0968 - val_loss: 0.0451 - val_mae: 0.1046\n",
            "Epoch 101/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0426 - mae: 0.0958 - val_loss: 0.0439 - val_mae: 0.1056\n",
            "Epoch 102/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0424 - mae: 0.0962 - val_loss: 0.0401 - val_mae: 0.0969\n",
            "Epoch 103/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0423 - mae: 0.0955 - val_loss: 0.0441 - val_mae: 0.1052\n",
            "Epoch 104/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0423 - mae: 0.0962 - val_loss: 0.0402 - val_mae: 0.0992\n",
            "Epoch 105/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0403 - mae: 0.0927 - val_loss: 0.0456 - val_mae: 0.0984\n",
            "Epoch 106/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0404 - mae: 0.0930 - val_loss: 0.0417 - val_mae: 0.0952\n",
            "Epoch 107/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0406 - mae: 0.0921 - val_loss: 0.0389 - val_mae: 0.0980\n",
            "Epoch 108/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0401 - mae: 0.0923 - val_loss: 0.0371 - val_mae: 0.0933\n",
            "Epoch 109/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0393 - mae: 0.0914 - val_loss: 0.0396 - val_mae: 0.0984\n",
            "Epoch 110/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0394 - mae: 0.0923 - val_loss: 0.0379 - val_mae: 0.0919\n",
            "Epoch 111/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0390 - mae: 0.0917 - val_loss: 0.0393 - val_mae: 0.0984\n",
            "Epoch 112/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0391 - mae: 0.0915 - val_loss: 0.0356 - val_mae: 0.0900\n",
            "Epoch 113/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0383 - mae: 0.0902 - val_loss: 0.0364 - val_mae: 0.0943\n",
            "Epoch 114/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0370 - mae: 0.0884 - val_loss: 0.0373 - val_mae: 0.0957\n",
            "Epoch 115/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0376 - mae: 0.0895 - val_loss: 0.0347 - val_mae: 0.0906\n",
            "Epoch 116/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0367 - mae: 0.0888 - val_loss: 0.0346 - val_mae: 0.0905\n",
            "Epoch 117/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0358 - mae: 0.0864 - val_loss: 0.0361 - val_mae: 0.0907\n",
            "Epoch 118/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0360 - mae: 0.0871 - val_loss: 0.0336 - val_mae: 0.0878\n",
            "Epoch 119/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0347 - mae: 0.0850 - val_loss: 0.0326 - val_mae: 0.0854\n",
            "Epoch 120/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0346 - mae: 0.0854 - val_loss: 0.0330 - val_mae: 0.0853\n",
            "Epoch 121/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0348 - mae: 0.0850 - val_loss: 0.0350 - val_mae: 0.0867\n",
            "Epoch 122/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0345 - mae: 0.0843 - val_loss: 0.0427 - val_mae: 0.1024\n",
            "Epoch 123/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0343 - mae: 0.0847 - val_loss: 0.0315 - val_mae: 0.0834\n",
            "Epoch 124/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0335 - mae: 0.0833 - val_loss: 0.0328 - val_mae: 0.0873\n",
            "Epoch 125/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0331 - mae: 0.0817 - val_loss: 0.0308 - val_mae: 0.0822\n",
            "Epoch 126/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.0811 - val_loss: 0.0304 - val_mae: 0.0806\n",
            "Epoch 127/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0321 - mae: 0.0812 - val_loss: 0.0300 - val_mae: 0.0796\n",
            "Epoch 128/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0312 - mae: 0.0793 - val_loss: 0.0331 - val_mae: 0.0861\n",
            "Epoch 129/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0314 - mae: 0.0797 - val_loss: 0.0307 - val_mae: 0.0827\n",
            "Epoch 130/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0310 - mae: 0.0797 - val_loss: 0.0326 - val_mae: 0.0866\n",
            "Epoch 131/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0309 - mae: 0.0792 - val_loss: 0.0295 - val_mae: 0.0798\n",
            "Epoch 132/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0296 - mae: 0.0764 - val_loss: 0.0288 - val_mae: 0.0762\n",
            "Epoch 133/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0292 - mae: 0.0755 - val_loss: 0.0303 - val_mae: 0.0792\n",
            "Epoch 134/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0293 - mae: 0.0760 - val_loss: 0.0277 - val_mae: 0.0752\n",
            "Epoch 135/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0284 - mae: 0.0756 - val_loss: 0.0317 - val_mae: 0.0804\n",
            "Epoch 136/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0285 - mae: 0.0745 - val_loss: 0.0275 - val_mae: 0.0742\n",
            "Epoch 137/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0280 - mae: 0.0757 - val_loss: 0.0268 - val_mae: 0.0740\n",
            "Epoch 138/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0273 - mae: 0.0742 - val_loss: 0.0274 - val_mae: 0.0742\n",
            "Epoch 139/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0270 - mae: 0.0730 - val_loss: 0.0275 - val_mae: 0.0758\n",
            "Epoch 140/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0269 - mae: 0.0733 - val_loss: 0.0275 - val_mae: 0.0761\n",
            "Epoch 141/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0257 - mae: 0.0709 - val_loss: 0.0300 - val_mae: 0.0794\n",
            "Epoch 142/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0254 - mae: 0.0701 - val_loss: 0.0268 - val_mae: 0.0720\n",
            "Epoch 143/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0255 - mae: 0.0700 - val_loss: 0.0266 - val_mae: 0.0721\n",
            "Epoch 144/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0250 - mae: 0.0692 - val_loss: 0.0257 - val_mae: 0.0705\n",
            "Epoch 145/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0249 - mae: 0.0679 - val_loss: 0.0263 - val_mae: 0.0718\n",
            "Epoch 146/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0251 - mae: 0.0672 - val_loss: 0.0280 - val_mae: 0.0761\n",
            "Epoch 147/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0247 - mae: 0.0674 - val_loss: 0.0264 - val_mae: 0.0722\n",
            "Epoch 148/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0241 - mae: 0.0668 - val_loss: 0.0253 - val_mae: 0.0677\n",
            "Epoch 149/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0237 - mae: 0.0663 - val_loss: 0.0244 - val_mae: 0.0665\n",
            "Epoch 150/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0228 - mae: 0.0638 - val_loss: 0.0254 - val_mae: 0.0706\n",
            "Epoch 151/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0225 - mae: 0.0641 - val_loss: 0.0236 - val_mae: 0.0646\n",
            "Epoch 152/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0218 - mae: 0.0637 - val_loss: 0.0232 - val_mae: 0.0645\n",
            "Epoch 153/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0224 - mae: 0.0641 - val_loss: 0.0247 - val_mae: 0.0679\n",
            "Epoch 154/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0213 - mae: 0.0627 - val_loss: 0.0224 - val_mae: 0.0635\n",
            "Epoch 155/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0213 - mae: 0.0610 - val_loss: 0.0218 - val_mae: 0.0612\n",
            "Epoch 156/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0215 - mae: 0.0612 - val_loss: 0.0214 - val_mae: 0.0610\n",
            "Epoch 157/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0205 - mae: 0.0605 - val_loss: 0.0218 - val_mae: 0.0597\n",
            "Epoch 158/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0209 - mae: 0.0590 - val_loss: 0.0218 - val_mae: 0.0624\n",
            "Epoch 159/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0196 - mae: 0.0587 - val_loss: 0.0244 - val_mae: 0.0680\n",
            "Epoch 160/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0202 - mae: 0.0583 - val_loss: 0.0212 - val_mae: 0.0595\n",
            "Epoch 161/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0198 - mae: 0.0571 - val_loss: 0.0208 - val_mae: 0.0578\n",
            "Epoch 162/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0198 - mae: 0.0561 - val_loss: 0.0207 - val_mae: 0.0589\n",
            "Epoch 163/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0190 - mae: 0.0553 - val_loss: 0.0206 - val_mae: 0.0561\n",
            "Epoch 164/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0187 - mae: 0.0539 - val_loss: 0.0214 - val_mae: 0.0585\n",
            "Epoch 165/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0184 - mae: 0.0538 - val_loss: 0.0201 - val_mae: 0.0564\n",
            "Epoch 166/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0192 - mae: 0.0534 - val_loss: 0.0223 - val_mae: 0.0624\n",
            "Epoch 167/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0189 - mae: 0.0536 - val_loss: 0.0195 - val_mae: 0.0541\n",
            "Epoch 168/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0183 - mae: 0.0524 - val_loss: 0.0210 - val_mae: 0.0572\n",
            "Epoch 169/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0183 - mae: 0.0519 - val_loss: 0.0243 - val_mae: 0.0608\n",
            "Epoch 170/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0184 - mae: 0.0510 - val_loss: 0.0242 - val_mae: 0.0617\n",
            "Epoch 171/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0190 - mae: 0.0519 - val_loss: 0.0205 - val_mae: 0.0558\n",
            "Epoch 172/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0181 - mae: 0.0506 - val_loss: 0.0214 - val_mae: 0.0566\n",
            "Epoch 173/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0175 - mae: 0.0501 - val_loss: 0.0209 - val_mae: 0.0542\n",
            "Epoch 174/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0174 - mae: 0.0492 - val_loss: 0.0219 - val_mae: 0.0582\n",
            "Epoch 175/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0172 - mae: 0.0482 - val_loss: 0.0238 - val_mae: 0.0574\n",
            "Epoch 176/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0174 - mae: 0.0481 - val_loss: 0.0205 - val_mae: 0.0540\n",
            "Epoch 177/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0172 - mae: 0.0485 - val_loss: 0.0185 - val_mae: 0.0495\n",
            "Epoch 178/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0177 - mae: 0.0491 - val_loss: 0.0205 - val_mae: 0.0541\n",
            "Epoch 179/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0161 - mae: 0.0461 - val_loss: 0.0202 - val_mae: 0.0510\n",
            "Epoch 180/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0165 - mae: 0.0458 - val_loss: 0.0194 - val_mae: 0.0527\n",
            "Epoch 181/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0171 - mae: 0.0470 - val_loss: 0.0212 - val_mae: 0.0551\n",
            "Epoch 182/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0164 - mae: 0.0463 - val_loss: 0.0192 - val_mae: 0.0497\n",
            "Epoch 183/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0166 - mae: 0.0456 - val_loss: 0.0200 - val_mae: 0.0517\n",
            "Epoch 184/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0165 - mae: 0.0446 - val_loss: 0.0197 - val_mae: 0.0516\n",
            "Epoch 185/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0158 - mae: 0.0447 - val_loss: 0.0193 - val_mae: 0.0511\n",
            "Epoch 186/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0162 - mae: 0.0442 - val_loss: 0.0202 - val_mae: 0.0504\n",
            "Epoch 187/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0158 - mae: 0.0432 - val_loss: 0.0174 - val_mae: 0.0446\n",
            "Epoch 188/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0164 - mae: 0.0439 - val_loss: 0.0176 - val_mae: 0.0456\n",
            "Epoch 189/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0155 - mae: 0.0436 - val_loss: 0.0179 - val_mae: 0.0454\n",
            "Epoch 190/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0156 - mae: 0.0436 - val_loss: 0.0177 - val_mae: 0.0459\n",
            "Epoch 191/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0156 - mae: 0.0426 - val_loss: 0.0184 - val_mae: 0.0478\n",
            "Epoch 192/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0152 - mae: 0.0419 - val_loss: 0.0211 - val_mae: 0.0527\n",
            "Epoch 193/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0155 - mae: 0.0424 - val_loss: 0.0175 - val_mae: 0.0450\n",
            "Epoch 194/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0160 - mae: 0.0423 - val_loss: 0.0177 - val_mae: 0.0443\n",
            "Epoch 195/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0157 - mae: 0.0417 - val_loss: 0.0196 - val_mae: 0.0507\n",
            "Epoch 196/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0154 - mae: 0.0418 - val_loss: 0.0173 - val_mae: 0.0430\n",
            "Epoch 197/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0403 - val_loss: 0.0171 - val_mae: 0.0421\n",
            "Epoch 198/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0153 - mae: 0.0407 - val_loss: 0.0210 - val_mae: 0.0501\n",
            "Epoch 199/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0148 - mae: 0.0406 - val_loss: 0.0229 - val_mae: 0.0515\n",
            "Epoch 200/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0148 - mae: 0.0397 - val_loss: 0.0181 - val_mae: 0.0445\n",
            "Epoch 201/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0158 - mae: 0.0401 - val_loss: 0.0174 - val_mae: 0.0411\n",
            "Epoch 202/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0155 - mae: 0.0398 - val_loss: 0.0182 - val_mae: 0.0438\n",
            "Epoch 203/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0153 - mae: 0.0397 - val_loss: 0.0179 - val_mae: 0.0443\n",
            "Epoch 204/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0145 - mae: 0.0387 - val_loss: 0.0199 - val_mae: 0.0448\n",
            "Epoch 205/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0389 - val_loss: 0.0174 - val_mae: 0.0413\n",
            "Epoch 206/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0151 - mae: 0.0384 - val_loss: 0.0191 - val_mae: 0.0466\n",
            "Epoch 207/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0146 - mae: 0.0370 - val_loss: 0.0201 - val_mae: 0.0465\n",
            "Epoch 208/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0368 - val_loss: 0.0163 - val_mae: 0.0390\n",
            "Epoch 209/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0377 - val_loss: 0.0184 - val_mae: 0.0420\n",
            "Epoch 210/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0373 - val_loss: 0.0181 - val_mae: 0.0417\n",
            "Epoch 211/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0378 - val_loss: 0.0162 - val_mae: 0.0378\n",
            "Epoch 212/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0149 - mae: 0.0370 - val_loss: 0.0163 - val_mae: 0.0389\n",
            "Epoch 213/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0357 - val_loss: 0.0176 - val_mae: 0.0440\n",
            "Epoch 214/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0373 - val_loss: 0.0190 - val_mae: 0.0427\n",
            "Epoch 215/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0151 - mae: 0.0372 - val_loss: 0.0165 - val_mae: 0.0387\n",
            "Epoch 216/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0148 - mae: 0.0364 - val_loss: 0.0173 - val_mae: 0.0388\n",
            "Epoch 217/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0149 - mae: 0.0372 - val_loss: 0.0167 - val_mae: 0.0378\n",
            "Epoch 218/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0359 - val_loss: 0.0165 - val_mae: 0.0375\n",
            "Epoch 219/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0356 - val_loss: 0.0162 - val_mae: 0.0375\n",
            "Epoch 220/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0356 - val_loss: 0.0213 - val_mae: 0.0455\n",
            "Epoch 221/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0354 - val_loss: 0.0187 - val_mae: 0.0450\n",
            "Epoch 222/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0146 - mae: 0.0371 - val_loss: 0.0165 - val_mae: 0.0375\n",
            "Epoch 223/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0354 - val_loss: 0.0213 - val_mae: 0.0453\n",
            "Epoch 224/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0357 - val_loss: 0.0184 - val_mae: 0.0423\n",
            "Epoch 225/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0142 - mae: 0.0349 - val_loss: 0.0181 - val_mae: 0.0397\n",
            "Epoch 226/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0347 - val_loss: 0.0187 - val_mae: 0.0412\n",
            "Epoch 227/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0355 - val_loss: 0.0161 - val_mae: 0.0364\n",
            "Epoch 228/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0337 - val_loss: 0.0183 - val_mae: 0.0423\n",
            "Epoch 229/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0340 - val_loss: 0.0174 - val_mae: 0.0389\n",
            "Epoch 230/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0334 - val_loss: 0.0201 - val_mae: 0.0437\n",
            "Epoch 231/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0338 - val_loss: 0.0302 - val_mae: 0.0577\n",
            "Epoch 232/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0351 - val_loss: 0.0157 - val_mae: 0.0352\n",
            "Epoch 233/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0142 - mae: 0.0346 - val_loss: 0.0165 - val_mae: 0.0393\n",
            "Epoch 234/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0336 - val_loss: 0.0159 - val_mae: 0.0351\n",
            "Epoch 235/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0331 - val_loss: 0.0160 - val_mae: 0.0363\n",
            "Epoch 236/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0145 - mae: 0.0340 - val_loss: 0.0195 - val_mae: 0.0404\n",
            "Epoch 237/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0333 - val_loss: 0.0165 - val_mae: 0.0362\n",
            "Epoch 238/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0143 - mae: 0.0336 - val_loss: 0.0183 - val_mae: 0.0387\n",
            "Epoch 239/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0332 - val_loss: 0.0161 - val_mae: 0.0345\n",
            "Epoch 240/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0312 - val_loss: 0.0151 - val_mae: 0.0360\n",
            "Epoch 241/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0335 - val_loss: 0.0149 - val_mae: 0.0346\n",
            "Epoch 242/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0143 - mae: 0.0336 - val_loss: 0.0153 - val_mae: 0.0370\n",
            "Epoch 243/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0135 - mae: 0.0327 - val_loss: 0.0167 - val_mae: 0.0369\n",
            "Epoch 244/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0143 - mae: 0.0328 - val_loss: 0.0183 - val_mae: 0.0398\n",
            "Epoch 245/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0322 - val_loss: 0.0156 - val_mae: 0.0332\n",
            "Epoch 246/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0317 - val_loss: 0.0184 - val_mae: 0.0406\n",
            "Epoch 247/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0340 - val_loss: 0.0156 - val_mae: 0.0348\n",
            "Epoch 248/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0131 - mae: 0.0316 - val_loss: 0.0168 - val_mae: 0.0349\n",
            "Epoch 249/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0136 - mae: 0.0317 - val_loss: 0.0166 - val_mae: 0.0357\n",
            "Epoch 250/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0138 - mae: 0.0317 - val_loss: 0.0202 - val_mae: 0.0415\n",
            "Epoch 251/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0312 - val_loss: 0.0150 - val_mae: 0.0349\n",
            "Epoch 252/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0313 - val_loss: 0.0213 - val_mae: 0.0436\n",
            "Epoch 253/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0311 - val_loss: 0.0177 - val_mae: 0.0364\n",
            "Epoch 254/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0317 - val_loss: 0.0150 - val_mae: 0.0367\n",
            "Epoch 255/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0136 - mae: 0.0314 - val_loss: 0.0174 - val_mae: 0.0390\n",
            "Epoch 256/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0321 - val_loss: 0.0157 - val_mae: 0.0355\n",
            "Epoch 257/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0319 - val_loss: 0.0185 - val_mae: 0.0389\n",
            "Epoch 258/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0314 - val_loss: 0.0146 - val_mae: 0.0322\n",
            "Epoch 259/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0314 - val_loss: 0.0146 - val_mae: 0.0333\n",
            "Epoch 260/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0315 - val_loss: 0.0183 - val_mae: 0.0376\n",
            "Epoch 261/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0323 - val_loss: 0.0148 - val_mae: 0.0343\n",
            "Epoch 262/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0307 - val_loss: 0.0198 - val_mae: 0.0423\n",
            "Epoch 263/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0305 - val_loss: 0.0131 - val_mae: 0.0313\n",
            "Epoch 264/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0310 - val_loss: 0.0256 - val_mae: 0.0501\n",
            "Epoch 265/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0315 - val_loss: 0.0141 - val_mae: 0.0316\n",
            "Epoch 266/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0125 - mae: 0.0318 - val_loss: 0.0147 - val_mae: 0.0361\n",
            "Epoch 267/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0303 - val_loss: 0.0139 - val_mae: 0.0317\n",
            "Epoch 268/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0306 - val_loss: 0.0147 - val_mae: 0.0324\n",
            "Epoch 269/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0303 - val_loss: 0.0147 - val_mae: 0.0314\n",
            "Epoch 270/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0303 - val_loss: 0.0149 - val_mae: 0.0317\n",
            "Epoch 271/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0296 - val_loss: 0.0138 - val_mae: 0.0318\n",
            "Epoch 272/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0136 - mae: 0.0313 - val_loss: 0.0156 - val_mae: 0.0346\n",
            "Epoch 273/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0125 - mae: 0.0308 - val_loss: 0.0134 - val_mae: 0.0326\n",
            "Epoch 274/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0133 - mae: 0.0312 - val_loss: 0.0132 - val_mae: 0.0311\n",
            "Epoch 275/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0307 - val_loss: 0.0138 - val_mae: 0.0315\n",
            "Epoch 276/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0304 - val_loss: 0.0227 - val_mae: 0.0462\n",
            "Epoch 277/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0299 - val_loss: 0.0146 - val_mae: 0.0326\n",
            "Epoch 278/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0295 - val_loss: 0.0128 - val_mae: 0.0307\n",
            "Epoch 279/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0125 - mae: 0.0288 - val_loss: 0.0175 - val_mae: 0.0379\n",
            "Epoch 280/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0286 - val_loss: 0.0173 - val_mae: 0.0362\n",
            "Epoch 281/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0291 - val_loss: 0.0164 - val_mae: 0.0372\n",
            "Epoch 282/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0296 - val_loss: 0.0160 - val_mae: 0.0343\n",
            "Epoch 283/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0296 - val_loss: 0.0145 - val_mae: 0.0319\n",
            "Epoch 284/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0284 - val_loss: 0.0197 - val_mae: 0.0413\n",
            "Epoch 285/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0290 - val_loss: 0.0129 - val_mae: 0.0296\n",
            "Epoch 286/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0288 - val_loss: 0.0127 - val_mae: 0.0296\n",
            "Epoch 287/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0295 - val_loss: 0.0138 - val_mae: 0.0300\n",
            "Epoch 288/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0134 - mae: 0.0299 - val_loss: 0.0153 - val_mae: 0.0340\n",
            "Epoch 289/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0280 - val_loss: 0.0133 - val_mae: 0.0282\n",
            "Epoch 290/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0282 - val_loss: 0.0127 - val_mae: 0.0282\n",
            "Epoch 291/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0284 - val_loss: 0.0137 - val_mae: 0.0296\n",
            "Epoch 292/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0282 - val_loss: 0.0157 - val_mae: 0.0340\n",
            "Epoch 293/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0281 - val_loss: 0.0137 - val_mae: 0.0295\n",
            "Epoch 294/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0276 - val_loss: 0.0130 - val_mae: 0.0285\n",
            "Epoch 295/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0273 - val_loss: 0.0170 - val_mae: 0.0365\n",
            "Epoch 296/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0280 - val_loss: 0.0156 - val_mae: 0.0334\n",
            "Epoch 297/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0280 - val_loss: 0.0134 - val_mae: 0.0298\n",
            "Epoch 298/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0276 - val_loss: 0.0153 - val_mae: 0.0338\n",
            "Epoch 299/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0270 - val_loss: 0.0189 - val_mae: 0.0377\n",
            "Epoch 300/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0282 - val_loss: 0.0135 - val_mae: 0.0289\n",
            "Epoch 301/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0277 - val_loss: 0.0141 - val_mae: 0.0330\n",
            "Epoch 302/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0276 - val_loss: 0.0143 - val_mae: 0.0309\n",
            "Epoch 303/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0274 - val_loss: 0.0133 - val_mae: 0.0314\n",
            "Epoch 304/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0270 - val_loss: 0.0134 - val_mae: 0.0294\n",
            "Epoch 305/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0128 - mae: 0.0286 - val_loss: 0.0135 - val_mae: 0.0270\n",
            "Epoch 306/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0261 - val_loss: 0.0135 - val_mae: 0.0279\n",
            "Epoch 307/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0281 - val_loss: 0.0130 - val_mae: 0.0270\n",
            "Epoch 308/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0258 - val_loss: 0.0125 - val_mae: 0.0292\n",
            "Epoch 309/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0273 - val_loss: 0.0140 - val_mae: 0.0296\n",
            "Epoch 310/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0274 - val_loss: 0.0142 - val_mae: 0.0302\n",
            "Epoch 311/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0265 - val_loss: 0.0134 - val_mae: 0.0291\n",
            "Epoch 312/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0271 - val_loss: 0.0134 - val_mae: 0.0298\n",
            "Epoch 313/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0264 - val_loss: 0.0169 - val_mae: 0.0345\n",
            "Epoch 314/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0278 - val_loss: 0.0128 - val_mae: 0.0277\n",
            "Epoch 315/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0267 - val_loss: 0.0127 - val_mae: 0.0271\n",
            "Epoch 316/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0261 - val_loss: 0.0140 - val_mae: 0.0289\n",
            "Epoch 317/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0270 - val_loss: 0.0124 - val_mae: 0.0273\n",
            "Epoch 318/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0268 - val_loss: 0.0139 - val_mae: 0.0306\n",
            "Epoch 319/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0266 - val_loss: 0.0218 - val_mae: 0.0399\n",
            "Epoch 320/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0270 - val_loss: 0.0127 - val_mae: 0.0285\n",
            "Epoch 321/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0263 - val_loss: 0.0198 - val_mae: 0.0405\n",
            "Epoch 322/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0263 - val_loss: 0.0128 - val_mae: 0.0295\n",
            "Epoch 323/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0262 - val_loss: 0.0155 - val_mae: 0.0319\n",
            "Epoch 324/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0265 - val_loss: 0.0191 - val_mae: 0.0365\n",
            "Epoch 325/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0255 - val_loss: 0.0178 - val_mae: 0.0337\n",
            "Epoch 326/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0263 - val_loss: 0.0187 - val_mae: 0.0359\n",
            "Epoch 327/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0271 - val_loss: 0.0136 - val_mae: 0.0291\n",
            "Epoch 328/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0257 - val_loss: 0.0132 - val_mae: 0.0265\n",
            "Epoch 329/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0259 - val_loss: 0.0138 - val_mae: 0.0268\n",
            "Epoch 330/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0259 - val_loss: 0.0140 - val_mae: 0.0307\n",
            "Epoch 331/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0249 - val_loss: 0.0171 - val_mae: 0.0345\n",
            "Epoch 332/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0256 - val_loss: 0.0133 - val_mae: 0.0271\n",
            "Epoch 333/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0261 - val_loss: 0.0147 - val_mae: 0.0290\n",
            "Epoch 334/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0264 - val_loss: 0.0133 - val_mae: 0.0265\n",
            "Epoch 335/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0258 - val_loss: 0.0125 - val_mae: 0.0263\n",
            "Epoch 336/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0255 - val_loss: 0.0135 - val_mae: 0.0274\n",
            "Epoch 337/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0260 - val_loss: 0.0163 - val_mae: 0.0315\n",
            "Epoch 338/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0250 - val_loss: 0.0185 - val_mae: 0.0339\n",
            "Epoch 339/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0260 - val_loss: 0.0176 - val_mae: 0.0356\n",
            "Epoch 340/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0251 - val_loss: 0.0138 - val_mae: 0.0297\n",
            "Epoch 341/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0108 - mae: 0.0252 - val_loss: 0.0124 - val_mae: 0.0259\n",
            "Epoch 342/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0250 - val_loss: 0.0134 - val_mae: 0.0294\n",
            "Epoch 343/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0251 - val_loss: 0.0156 - val_mae: 0.0311\n",
            "Epoch 344/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0252 - val_loss: 0.0145 - val_mae: 0.0293\n",
            "Epoch 345/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0243 - val_loss: 0.0138 - val_mae: 0.0295\n",
            "Epoch 346/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0246 - val_loss: 0.0128 - val_mae: 0.0275\n",
            "Epoch 347/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0243 - val_loss: 0.0135 - val_mae: 0.0286\n",
            "Epoch 348/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0244 - val_loss: 0.0174 - val_mae: 0.0336\n",
            "Epoch 349/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0108 - mae: 0.0245 - val_loss: 0.0145 - val_mae: 0.0277\n",
            "Epoch 350/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0236 - val_loss: 0.0138 - val_mae: 0.0294\n",
            "Epoch 351/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0247 - val_loss: 0.0132 - val_mae: 0.0266\n",
            "Epoch 352/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0247 - val_loss: 0.0144 - val_mae: 0.0277\n",
            "Epoch 353/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0248 - val_loss: 0.0143 - val_mae: 0.0285\n",
            "Epoch 354/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0248 - val_loss: 0.0224 - val_mae: 0.0409\n",
            "Epoch 355/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0252 - val_loss: 0.0126 - val_mae: 0.0273\n",
            "Epoch 356/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0258 - val_loss: 0.0135 - val_mae: 0.0266\n",
            "Epoch 357/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0236 - val_loss: 0.0162 - val_mae: 0.0325\n",
            "Epoch 358/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0230 - val_loss: 0.0154 - val_mae: 0.0317\n",
            "Epoch 359/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0248 - val_loss: 0.0164 - val_mae: 0.0315\n",
            "Epoch 360/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0239 - val_loss: 0.0139 - val_mae: 0.0287\n",
            "Epoch 361/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0233 - val_loss: 0.0163 - val_mae: 0.0313\n",
            "Epoch 362/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0238 - val_loss: 0.0151 - val_mae: 0.0299\n",
            "Epoch 363/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0238 - val_loss: 0.0178 - val_mae: 0.0362\n",
            "Epoch 364/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0239 - val_loss: 0.0127 - val_mae: 0.0266\n",
            "Epoch 365/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0239 - val_loss: 0.0135 - val_mae: 0.0257\n",
            "Epoch 366/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0233 - val_loss: 0.0125 - val_mae: 0.0255\n",
            "Epoch 367/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0229 - val_loss: 0.0128 - val_mae: 0.0267\n",
            "Epoch 368/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0239 - val_loss: 0.0166 - val_mae: 0.0305\n",
            "Epoch 369/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0108 - mae: 0.0237 - val_loss: 0.0152 - val_mae: 0.0293\n",
            "Epoch 370/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0239 - val_loss: 0.0165 - val_mae: 0.0325\n",
            "Epoch 371/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0239 - val_loss: 0.0132 - val_mae: 0.0263\n",
            "Epoch 372/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0235 - val_loss: 0.0122 - val_mae: 0.0248\n",
            "Epoch 373/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0239 - val_loss: 0.0190 - val_mae: 0.0366\n",
            "Epoch 374/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0234 - val_loss: 0.0127 - val_mae: 0.0251\n",
            "Epoch 375/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0224 - val_loss: 0.0120 - val_mae: 0.0259\n",
            "Epoch 376/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0232 - val_loss: 0.0136 - val_mae: 0.0276\n",
            "Epoch 377/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0234 - val_loss: 0.0265 - val_mae: 0.0478\n",
            "Epoch 378/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0240 - val_loss: 0.0189 - val_mae: 0.0365\n",
            "Epoch 379/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0228 - val_loss: 0.0155 - val_mae: 0.0305\n",
            "Epoch 380/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0103 - mae: 0.0225 - val_loss: 0.0221 - val_mae: 0.0397\n",
            "Epoch 381/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0244 - val_loss: 0.0123 - val_mae: 0.0258\n",
            "Epoch 382/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0229 - val_loss: 0.0159 - val_mae: 0.0304\n",
            "Epoch 383/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0230 - val_loss: 0.0129 - val_mae: 0.0240\n",
            "Epoch 384/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0225 - val_loss: 0.0130 - val_mae: 0.0256\n",
            "Epoch 385/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0235 - val_loss: 0.0135 - val_mae: 0.0252\n",
            "Epoch 386/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0104 - mae: 0.0231 - val_loss: 0.0117 - val_mae: 0.0234\n",
            "Epoch 387/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0222 - val_loss: 0.0141 - val_mae: 0.0269\n",
            "Epoch 388/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0223 - val_loss: 0.0123 - val_mae: 0.0239\n",
            "Epoch 389/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0224 - val_loss: 0.0127 - val_mae: 0.0252\n",
            "Epoch 390/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0222 - val_loss: 0.0167 - val_mae: 0.0332\n",
            "Epoch 391/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0225 - val_loss: 0.0136 - val_mae: 0.0277\n",
            "Epoch 392/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0108 - mae: 0.0224 - val_loss: 0.0144 - val_mae: 0.0279\n",
            "Epoch 393/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0224 - val_loss: 0.0129 - val_mae: 0.0271\n",
            "Epoch 394/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0230 - val_loss: 0.0171 - val_mae: 0.0315\n",
            "Epoch 395/400\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.0109 - mae: 0.0226 - val_loss: 0.0135 - val_mae: 0.0262\n",
            "Epoch 396/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0225 - val_loss: 0.0154 - val_mae: 0.0297\n",
            "Epoch 397/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0103 - mae: 0.0212 - val_loss: 0.0118 - val_mae: 0.0228\n",
            "Epoch 398/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0102 - mae: 0.0206 - val_loss: 0.0162 - val_mae: 0.0315\n",
            "Epoch 399/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0223 - val_loss: 0.0128 - val_mae: 0.0253\n",
            "Epoch 400/400\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0225 - val_loss: 0.0139 - val_mae: 0.0259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMjtfa42ahM"
      },
      "source": [
        "### Run with Test Data\n",
        "Put our test data into the model and plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Y0CCWJz2EK",
        "outputId": "8b1543c4-cf4d-443e-c87b-bfb6f9e87861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "\n",
        "# Plot the predictions along with to the test data\n",
        "plt.clf()\n",
        "plt.title('Training data predicted vs actual values')\n",
        "plt.plot(inputs_test, outputs_test, 'b.', label='Actual')\n",
        "plt.plot(inputs_test, predictions, 'r.', label='Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions =\n",
            " [[0.    0.999 0.    0.001]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.013 0.987 0.    0.   ]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.    0.065 0.    0.935]\n",
            " [0.002 0.998 0.    0.   ]\n",
            " [0.    0.    0.    0.999]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.001 0.999 0.    0.   ]\n",
            " [0.982 0.018 0.    0.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.995 0.005 0.    0.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [1.    0.    0.    0.   ]\n",
            " [0.    0.    0.012 0.988]\n",
            " [0.007 0.993 0.    0.   ]\n",
            " [0.    0.008 0.    0.992]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.005 0.992 0.    0.003]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.987 0.013 0.    0.   ]\n",
            " [0.262 0.738 0.    0.   ]\n",
            " [0.    0.135 0.001 0.864]\n",
            " [0.049 0.947 0.    0.004]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.001 0.999]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.    0.302 0.    0.698]\n",
            " [0.    0.005 0.    0.994]\n",
            " [0.    0.    0.898 0.102]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.    0.98  0.02 ]\n",
            " [0.    0.    0.    0.999]\n",
            " [0.    0.    0.973 0.027]\n",
            " [0.    0.    0.974 0.026]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.002 0.    0.998]\n",
            " [0.001 0.999 0.    0.   ]\n",
            " [0.912 0.088 0.    0.   ]\n",
            " [0.    0.023 0.    0.977]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.    0.997 0.003]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.993 0.007 0.    0.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.991 0.009]\n",
            " [0.    0.001 0.931 0.068]\n",
            " [0.926 0.074 0.    0.   ]\n",
            " [0.    0.012 0.    0.988]\n",
            " [0.398 0.602 0.    0.   ]\n",
            " [0.    0.    0.541 0.459]\n",
            " [0.    0.    0.999 0.001]\n",
            " [0.995 0.005 0.    0.   ]\n",
            " [0.994 0.006 0.    0.   ]\n",
            " [0.    0.    0.999 0.001]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [0.139 0.861 0.    0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.    0.998 0.002]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.001 0.999 0.    0.   ]\n",
            " [0.987 0.013 0.    0.   ]\n",
            " [0.775 0.222 0.    0.003]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.    0.001 0.    0.999]\n",
            " [0.    0.473 0.003 0.524]\n",
            " [0.    0.    0.999 0.001]\n",
            " [0.004 0.996 0.    0.   ]\n",
            " [0.    0.957 0.    0.043]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.995 0.005 0.    0.   ]\n",
            " [0.    0.    0.898 0.102]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.991 0.009 0.    0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.001 0.    0.999]\n",
            " [0.    0.91  0.    0.09 ]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.    0.    0.979 0.021]\n",
            " [0.    0.    0.998 0.002]\n",
            " [0.    0.    0.001 0.999]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.994 0.006 0.    0.   ]\n",
            " [0.    0.    0.992 0.008]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.001 0.    0.999]\n",
            " [0.    0.997 0.    0.003]\n",
            " [0.007 0.992 0.    0.001]\n",
            " [0.002 0.998 0.    0.   ]\n",
            " [0.001 0.999 0.    0.   ]\n",
            " [0.993 0.007 0.    0.   ]\n",
            " [0.    0.    0.993 0.007]\n",
            " [1.    0.    0.    0.   ]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [0.004 0.996 0.    0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.001 0.005 0.994]\n",
            " [0.    0.    0.012 0.988]\n",
            " [0.987 0.013 0.    0.   ]\n",
            " [0.    0.    0.004 0.996]\n",
            " [0.    0.    0.974 0.026]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.    0.    0.012 0.988]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.016 0.984 0.    0.   ]\n",
            " [0.    0.006 0.001 0.993]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.98  0.02  0.    0.   ]\n",
            " [0.    0.041 0.    0.959]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.85  0.15  0.    0.   ]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.013 0.987 0.    0.   ]\n",
            " [0.    0.    0.999 0.001]\n",
            " [0.    0.003 0.001 0.996]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.144 0.854 0.    0.001]\n",
            " [0.    0.997 0.    0.003]\n",
            " [0.    0.    0.997 0.003]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.    0.    0.972 0.028]\n",
            " [0.982 0.018 0.    0.   ]\n",
            " [0.    0.    0.999 0.001]\n",
            " [0.    0.001 0.95  0.049]\n",
            " [0.    0.217 0.001 0.782]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [1.    0.    0.    0.   ]\n",
            " [0.    0.    0.997 0.003]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.01  0.    0.99 ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.452 0.535 0.    0.013]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.994 0.006 0.    0.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.995 0.005]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.008 0.991 0.    0.   ]\n",
            " [0.952 0.048 0.    0.   ]\n",
            " [0.    0.    0.994 0.006]\n",
            " [0.08  0.852 0.    0.067]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.014 0.    0.986]\n",
            " [0.    0.    0.993 0.007]\n",
            " [0.897 0.103 0.    0.   ]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [0.    0.009 0.    0.991]\n",
            " [0.    0.997 0.    0.003]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.    0.003 0.001 0.996]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.823 0.177 0.    0.   ]\n",
            " [0.983 0.017 0.    0.   ]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.    0.    0.999 0.001]\n",
            " [0.979 0.021 0.    0.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.001 0.997 0.    0.002]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.    0.    0.988 0.012]\n",
            " [0.    0.004 0.    0.996]\n",
            " [0.    0.003 0.    0.997]\n",
            " [0.    0.997 0.    0.003]\n",
            " [0.001 0.997 0.    0.002]\n",
            " [0.    0.    1.    0.   ]\n",
            " [1.    0.    0.    0.   ]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.    0.945 0.    0.055]\n",
            " [0.    0.001 0.    0.999]\n",
            " [0.997 0.003 0.    0.   ]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.    0.004 0.996]\n",
            " [0.    0.    0.98  0.02 ]\n",
            " [0.    0.001 0.    0.999]\n",
            " [0.    0.001 0.113 0.886]\n",
            " [0.    0.003 0.    0.997]\n",
            " [0.984 0.016 0.    0.   ]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.    0.    0.017 0.983]\n",
            " [0.998 0.002 0.    0.   ]\n",
            " [0.244 0.732 0.    0.023]\n",
            " [0.001 0.999 0.    0.   ]\n",
            " [0.    0.002 0.001 0.997]\n",
            " [0.    0.    0.    1.   ]\n",
            " [0.    0.979 0.    0.021]\n",
            " [0.    0.006 0.003 0.991]\n",
            " [0.    0.    0.998 0.002]\n",
            " [0.    0.    0.993 0.007]\n",
            " [0.996 0.004 0.    0.   ]\n",
            " [0.936 0.064 0.    0.   ]\n",
            " [0.051 0.948 0.    0.   ]\n",
            " [0.    0.01  0.001 0.989]\n",
            " [0.    0.    0.999 0.001]]\n",
            "actual =\n",
            " [[0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAABlCAYAAABZcXdQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "# Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xn1-Rn9Cp_8",
        "outputId": "307b5e12-0312-4d42-9225-ff90e9eaa4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "  \n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is 3184 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "## Encode the Model in an Arduino Header File \n",
        "\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J33uwpNtAku",
        "outputId": "79bb5320-ad26-48e1-cacd-1708581f9964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: xxd: command not found\n",
            "Header file, model.h, is 35 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Realtime Classification of Sensor Data on Arduino\n",
        "\n",
        "Now it's time to switch back to the tutorial instructions and run our new model on the [Arduino Nano 33 BLE Sense](https://www.arduino.cc/en/Guide/NANO33BLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_eaWqgpAlC4"
      },
      "source": [
        "# Nouvelle section"
      ]
    }
  ]
}